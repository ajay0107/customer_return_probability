{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"table-of-contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Section 1: Install Packages](#Install-Packages)\n",
    "* [Section 2: Load Data](#load-data)\n",
    "* [Section 3: Summary of Data](#summary-of-data)\n",
    "    * [Section 3.1: Exploratory Data Analysis (EDA) of Order Data](#section_3_1)\n",
    "        * [Section 3.1.1: Data Shape and Structure](#section_3_1_1)\n",
    "        * [Section 3.1.2: Summary of Numerical Columns](#section_3_1_2)\n",
    "        * [Section 3.1.3: Number of Unique Ids](#section_3_1_3)\n",
    "        * [Section 3.1.4: Imputing Missing Data](#section_3_1_4)\n",
    "        * [Section 3.1.5: Outlier Detection and Removal](#section_3_1_5)\n",
    "        * [Section 3.1.6: Trend of Orders with Time](#section_3_1_6)\n",
    "        * [Section 3.1.7: Deciding Time Range of Order Data](#section_3_1_7)\n",
    "    * [Section 3.2: Basic Details of Labeled Data](#section_3_2)\n",
    "* [Section 4: Feature Engineering](#section_4)\n",
    "    * [Section 4.1: Removing Failed Orders before Feature Engineering](#section_4_1)\n",
    "    * [Section 4.2: Creating New Features](#section_4_2)\n",
    "        * [Section 4.2.1: Total number of orders by each customers(Frequency (F)) ](#section_4_2_1)\n",
    "        * [Section 4.2.2: Total Order Value by Each Customer](#section_4_2_2)\n",
    "        * [Section 4.2.3: Total amount paid in last 180 days](#section_4_2_3)\n",
    "        * [Section 4.2.5: Number of unique restaurants customer ordered from](#section_4_2_5)\n",
    "        * [Section 4.2.6: Recency, Customer Maturity, Percentage of spending in 2nd half ](#section_4_2_6)\n",
    "        * [Section 4.2.7: Maximum, Mean, Median Value of Orders](#section_4_2_7)\n",
    "        * [Section 4.2.8: Customer Segmentation into Single Order,Normal,Attrition,At-Risk,Lost](#section_4_2_8)\n",
    "        * [Section 4.2.9: Check if id columns correlated with Customer Return](#section_4_2_9)\n",
    "        * [Section 4.2.10: Creating New Features as Pairwise Products of Numerical Variables](#section_4_2_10)\n",
    "        * [Section 4.2.11: Creating New Features as square of Numerical Variables](#section_4_2_11)\n",
    "        \n",
    "* [Section 5: Modelling](#section_5)\n",
    "    * [Section 5.1: One-Hot Encoding of Nominal Categorical Variables](#section_5_1)\n",
    "    * [Section 5.2: Train,Test Split of Dataset](#section_5_2)\n",
    "    * [Section 5.3: Tackling imbalance class problem](#section_5_3)\n",
    "    * [Section 5.4: XGBoost Model Development and Evaluation](#section_5_4)\n",
    "        * [Section 5.4.1: XGBoost Hyperparameter Grid Search](#section_5_4_1)\n",
    "    * [Section 5.5: Logistic Regression Model Development and Evaluation](#section_5_5)\n",
    "    * [Section 5.6: Random Forest Model Development and Evaluation](#section_5_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages <a class=\"anchor\" id=\"Install-Packages\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions definiton\n",
    "\n",
    "def num_unique_values(df):\n",
    "    \"\"\"\n",
    "    This function takes dataframe and returns number of unique values in each column\n",
    "    :param df: dataframe\n",
    "    :return: dataframe of names of columns and number of unique values in each columns\n",
    "    \"\"\"\n",
    "    col_names = []\n",
    "    values = []\n",
    "    for col in df.columns:\n",
    "        col_names.append(col)\n",
    "        values.append(len(df[col].unique()))\n",
    "    return(pd.DataFrame({'col_names':col_names,'num_unique_values':values}))\n",
    "\n",
    "\n",
    "def perc_return_customers(df, bin_col,bins,title,xlab,ylab,bins_range, fontsize):\n",
    "    \"\"\"\n",
    "    This function takes dataframe with bins and plots bar plot of returning customers with x-axis\n",
    "    :param df: dataframe with bins\n",
    "    :param bin_col: name of column containing bins\n",
    "    :param bins: bins range\n",
    "    :param title: title of plot\n",
    "    :param xlab: x-label of plot\n",
    "    :param ylab: y-label of plot\n",
    "    :param bins_range: whether bins range is provided 'auto' or 'manual'\n",
    "    :param fontsize: fontsize of title,labels\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df = df.groupby([bin_col],as_index= False).agg({'customer_id':'count','is_returning_customer':'sum'})\n",
    "    # calculating percentage of returning customers\n",
    "    df['perc_return_cust'] = (df['is_returning_customer']/df['customer_id'])*100\n",
    "    # plotting bar graph\n",
    "    if bins_range == 'auto':\n",
    "        df['bin_index'] = range(1,bins+1,1)\n",
    "    else:\n",
    "        df['bin_index'] = range(1,len(bins),1)\n",
    "    plt.bar(df['bin_index'],df['perc_return_cust'] )\n",
    "    plt.xticks(df['bin_index'],df[bin_col])\n",
    "    plt.title(title, fontweight='bold',fontsize=fontsize)\n",
    "    plt.xlabel(xlab, fontsize=fontsize)\n",
    "    plt.ylabel(ylab,fontsize=fontsize)\n",
    "    return None\n",
    "\n",
    "def Percentage_spending_2nd_half(df,starting_date,last_date):\n",
    "    '''\n",
    "    This function gives relative change in spending in 1st half and 2nd half of observation period\n",
    "    :param df: dataframe\n",
    "    :param starting_date: starting date of observation period\n",
    "    :param last_date: Last date of observation period\n",
    "    :return: dataframe containing relative change\n",
    "    '''\n",
    "    mid_date = starting_date + ((last_date - starting_date)/2)\n",
    "    df1 = df.loc[(df['order_date'] <= mid_date),:].groupby(['customer_id'],as_index=False).agg({'amount_paid':sum})\n",
    "    df1 = df1.rename({'amount_paid':'amount_paid_1st_half'},axis=1)\n",
    "    df2 = df.loc[df['order_date'] > mid_date,:].groupby(['customer_id'],as_index=False).agg({'amount_paid':sum})\n",
    "    df2 = df2.rename({'amount_paid':'amount_paid_2nd_half'},axis=1)\n",
    "    df1 = df1.merge(df2,on=['customer_id'],how='outer')\n",
    "    df1.fillna(0,inplace=True)\n",
    "    df1['total_amount'] = df1['amount_paid_1st_half'] + df1['amount_paid_2nd_half']\n",
    "    df1['perc_amount_paid_2nd_half'] = (df1['amount_paid_2nd_half']/df1['total_amount'])*100\n",
    "    return df1\n",
    "\n",
    "def customer_classification(df):\n",
    "    '''\n",
    "    we segment customers into single_order, normal, attrition, at_risk, lost based on their mean time units between orders and standard deviation of time units between orders as described below.\n",
    "    only single purchase in dataset: single_order customer\n",
    "    time since last order <= mean + 2*sd : Normal customer\n",
    "    mean + 2*sd < time since last order <= mean + 4*sd : attrition customer\n",
    "    mean + 4*sd < time since last order <= mean + 8*sd : at_risk customer\n",
    "    time since last order > mean + 8*sd : lost customer\n",
    "    :param df: input dataframe containing records of customers orders\n",
    "    :return: dataframe containing customers ids and their classification\n",
    "    '''\n",
    "    temp_df = df.groupby(['customer_id'],as_index=False).agg({'order_date':'count'}) # creating temporary df for manipulation\n",
    "    temp_df.columns = ['customer_id','num_of_orders']\n",
    "    single_order_customers = temp_df.loc[temp_df['num_of_orders'] == 1,:]\n",
    "    single_order_customers['customer_type'] = 'single_order'\n",
    "    multi_order_cust = temp_df.loc[temp_df['num_of_orders'] > 1,:]\n",
    "    multi_order_cust =  df.loc[(df['customer_id'].isin(multi_order_cust['customer_id'])),:]\n",
    "    # calculating time difference between consecutive orders for multiple order customers\n",
    "    multi_order_cust = multi_order_cust.assign(timediff=multi_order_cust.sort_values('order_date', ascending=True).groupby(['customer_id']).order_date.diff(1).dt.days.fillna(0))\n",
    "    time_since_last_order = multi_order_cust.groupby(['customer_id'],as_index=False).agg({'order_date':'max'}) # time since last order\n",
    "    time_since_last_order['time_since_last_purchase'] = (last_date - time_since_last_order['order_date']).dt.days\n",
    "    multi_order_cust = multi_order_cust.groupby(['customer_id'],as_index=False).agg({'timediff':['mean','std']})\n",
    "    multi_order_cust.columns = ['customer_id','timediff_mean','timediff_std']\n",
    "    # categorizing multiple order customers into normal,attrition, at_risk, lost\n",
    "    multi_order_cust = multi_order_cust.merge(time_since_last_order[['customer_id','time_since_last_purchase']], on = ['customer_id'], how='inner')\n",
    "    multi_order_cust['mean_2std'] = multi_order_cust['timediff_mean']+2*multi_order_cust['timediff_std']\n",
    "    multi_order_cust['mean_4std'] = multi_order_cust['timediff_mean']+4*multi_order_cust['timediff_std']\n",
    "    multi_order_cust['mean_8std'] = multi_order_cust['timediff_mean']+8*multi_order_cust['timediff_std']\n",
    "    multi_order_cust.loc[(multi_order_cust['time_since_last_purchase'] <= multi_order_cust['mean_2std']),'customer_type'] = 'normal'\n",
    "    multi_order_cust.loc[(multi_order_cust['time_since_last_purchase'] > multi_order_cust['mean_2std']) & (multi_order_cust['time_since_last_purchase'] <= multi_order_cust['mean_4std']),'customer_type'] = 'attrition'\n",
    "    multi_order_cust.loc[(multi_order_cust['time_since_last_purchase'] > multi_order_cust['mean_4std']) & (multi_order_cust['time_since_last_purchase'] <= multi_order_cust['mean_8std']),'customer_type'] = 'at_risk'\n",
    "    multi_order_cust.loc[(multi_order_cust['time_since_last_purchase'] > multi_order_cust['mean_8std']),'customer_type'] = 'lost'\n",
    "    # combine single order customers with multiple order customers\n",
    "    req_cols = ['customer_id','customer_type']\n",
    "    combined_df = multi_order_cust[req_cols].append(single_order_customers[req_cols]).reset_index(drop=True)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def perc_return_cust_by_ids(df,id_name):\n",
    "    '''\n",
    "    This function takes dataframe and gives percentage of returning customers by specific id values\n",
    "    :param df: input order details dataframe\n",
    "    :param id_name: name of id column\n",
    "    :return: returns dataframe with percentage of returning customers\n",
    "    \n",
    "    '''\n",
    "    temp = df.groupby(['customer_id',id_name],as_index=False).agg({'order_date':'count'})\n",
    "    temp.sort_values(['customer_id','order_date'],ascending=False,inplace=True)\n",
    "    temp.drop_duplicates(['customer_id'],keep = 'first',inplace=True)\n",
    "    temp = temp.merge(df_labeled[['customer_id','is_returning_customer']],on=['customer_id'],how='inner')\n",
    "    temp = temp.groupby([id_name],as_index=False).agg({'customer_id':'count','is_returning_customer':'sum'})\n",
    "    temp.columns = [id_name,'total_num_customers','num_return_customers']\n",
    "    temp['return_perc'] = (temp['num_return_customers']/temp['total_num_customers'])*100\n",
    "    return temp\n",
    "    \n",
    "def model_evaluation_metrices(y_true,y_pred):\n",
    "    '''\n",
    "    This function gives model evaluation metrices viz. cm,accuracy,sensitivity,specificity,auc,f1_score \n",
    "    for binary classification\n",
    "    :param y_true: true target labels\n",
    "    :param y_pred: prediction target labels\n",
    "    :return: model evaluation metrices viz. cm,accuracy,sensitivity,specificity,auc,f1_score\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    auc = roc_auc_score(y_true,y_pred)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    accuracy = (tp + tn)/(tn+fp+fn+tp)\n",
    "    f1_score = (2*tp)/(2*tp+fp+fn)\n",
    "    return cm,accuracy,sensitivity,specificity,auc,f1_score   \n",
    "\n",
    "def undersampling_function(X_train,y_train, num_major_class):\n",
    "    X_train.reset_index(drop=True,inplace=True)\n",
    "    y_train.reset_index(drop=True,inplace=True)\n",
    "    train_temp = pd.concat([X_train,y_train],axis=1) #temporary train df\n",
    "    target_zero =train_temp.loc[train_temp['is_returning_customer']==0,:]\n",
    "    target_one =train_temp.loc[train_temp['is_returning_customer']==1,:]\n",
    "    target_zero_undersampled = target_zero.sample(n=num_major_class) # taking only 50k positive samples\n",
    "    train_temp = pd.concat([target_zero_undersampled,target_one],axis=0)\n",
    "    train_temp.reset_index(drop=True,inplace=True)\n",
    "    X_train = train_temp.loc[:,(~pd.Series(train_temp.columns).isin(['is_returning_customer'])).tolist()]\n",
    "    y_train = train_temp.loc[:,['is_returning_customer']]\n",
    "    return X_train,y_train\n",
    "\n",
    "def pairwise_products(df,num_cols):\n",
    "    list_pairwise_cols = list(itertools.combinations(num_cols, 2))\n",
    "    count=1\n",
    "    for i in list_pairwise_cols:\n",
    "        df['pair{}'.format(count)] = df[i[0]]*df[i[1]]\n",
    "        count+=1\n",
    "    return df\n",
    "    \n",
    "def square_numerical_cols(df,num_cols):\n",
    "    for i in num_cols:\n",
    "        df['square_{}'.format(i)] = df[i]*df[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data <a class=\"anchor\" id=\"load-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "df_order = pd.read_csv('data/machine_learning_challenge_order_data.csv',index_col=False) # loading order data\n",
    "df_labeled = pd.read_csv('data/machine_learning_challenge_labeled_data.csv',index_col=False) # loading labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) <a class=\"anchor\" id=\"summary-of-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) of Order Data <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Shape and Structure <a class=\"anchor\" id=\"section_3_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get glimpse of order data\n",
    "print(df_order.shape)\n",
    "df_order.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Numerical Columns <a class=\"anchor\" id=\"section_3_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split columns into 3 categories numerical, id and date columns for future use\n",
    "numerical_cols = ['order_hour', 'customer_order_rank','is_failed','voucher_amount', 'delivery_fee', 'amount_paid']\n",
    "id_cols = ['customer_id','restaurant_id', 'city_id', 'payment_id', 'platform_id','transmission_id']\n",
    "date_cols = ['order_date']\n",
    "# summary of numerical columns of order data\n",
    "df_order.loc[:,numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Unique Ids <a class=\"anchor\" id=\"section_3_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of unique ids in order data\n",
    "num_unique_values(df_order[id_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing Missing Data <a class=\"anchor\" id=\"section_3_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking number of null values in order data\n",
    "df_order.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**customer_order_rank** contains **24761** null values and all other columns doesn't contain any null values. We need to why is there null values in customer_order_rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FInding reason for customer having null values in order rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Records where customer_order_rank is null\n",
    "df_order_rank_null = df_order.loc[df_order['customer_order_rank'].isnull(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`It seems like where ever order was failed, customer order rank was left blank which makes sense. Let's verify, is all combinations of customer_order_rank x is_failed is NAN x 1 ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying if there exist only one combination of NAN x 1\n",
    "df_order_rank_null.drop_duplicates(subset = ['customer_order_rank','is_failed'],keep ='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's verified that **customer_order_rank is null, where ever order was failed**. We don't need to impute data as missing values in customer order rank is due to valid reason and not because of missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection and Removal <a class=\"anchor\" id=\"section_3_1_5\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Discovering outliers with visualization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if order hour lies between 0 - 23 for all records\n",
    "print('Range of Order Hour is : \\n{} to {}\\n'.format(df_order['order_hour'].min(),df_order['order_hour'].max()))\n",
    "# Time range of Order data\n",
    "# convert date into datetime datatype\n",
    "df_order['order_date'] = pd.to_datetime(df_order['order_date'])\n",
    "# get range of order date\n",
    "print('Range of order dates : \\n {} - {}\\n'.format(min(df_order['order_date']),max(df_order['order_date'])))\n",
    "# Unique Value of is_failed\n",
    "print('Unique values in is_failed : \\n{}'.format(df_order['is_failed'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`There is no outlier in Order Hour, Order Date and is_failed columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting boxplots for relevant numerical variables in order data\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,10))\n",
    "axs[0, 0].boxplot(df_order['voucher_amount'])\n",
    "axs[0, 0].set_title('Boxplot Voucher Amount')\n",
    "axs[0, 1].boxplot(df_order['delivery_fee'])\n",
    "axs[0, 1].set_title('Boxplot Delivery Fee')\n",
    "axs[1, 0].boxplot(df_order['amount_paid'])\n",
    "axs[1, 0].set_title('Boxplot Amount Paid')\n",
    "axs[1, 1].boxplot(df_order['customer_order_rank'][~df_order['customer_order_rank'].isnull()])\n",
    "axs[1, 1].set_title('Boxplot Customer Order Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount paid is greater than 500 in two cases,but they may not be an outlier. It may be an order on **special occasion** like \n",
    "Birthday Party, Anniversary etc when people order in large quantities, so we are not categorizing it as Outlier. **We don't find any outlier in voucher_amount, delivery_fee, amount_paid and customer order rank**. All are in reasonable range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding Time Range of Order Data <a class=\"anchor\" id=\"section_3_1_7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend of Orders with Time <a class=\"anchor\" id=\"section_3_1_6\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trend of orders with respect to time (i.e. number of orders each day)\n",
    "df_orders_by_time = df_order.groupby(['order_date'], as_index = False).size().reset_index(name='count')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_orders_by_time['order_date'],df_orders_by_time['count'])\n",
    "plt.title('Number of Orders with time', fontsize = 15, fontweight='bold' )\n",
    "plt.xlabel('Time', fontsize = 15)\n",
    "plt.ylabel('Number of Orders', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose time-frame for our orders data. From above graph, We can see that there are negligible orders before **March 2015**. We need to exactly find this date after which significant orders started. From EDA, we can say that let's take starting date of data at which first time we received atlleast 100 orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find date after which significant orders started\n",
    "starting_date = df_orders_by_time.loc[df_orders_by_time['count'] > 100,:].reset_index()['order_date'][0]\n",
    "print('Date after which significant orders started (actual starting date of order data) : {}'.format(starting_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data points before 2015-03-01\n",
    "df_order = df_order.loc[df_order['order_date'] >= starting_date,:]\n",
    "# Let's see how number of orders with time look like\n",
    "df_orders_by_time = df_order.groupby(['order_date'], as_index = False).size().reset_index(name='count')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_orders_by_time['order_date'],df_orders_by_time['count'])\n",
    "plt.title('Number of Orders with time after removing negligible orders', fontsize = 15, fontweight='bold' )\n",
    "plt.xlabel('Time', fontsize = 15)\n",
    "plt.ylabel('Number of Orders', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Distribution of total number of orders by each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Distribution of total number of orders by each customer\n",
    "df_num_orders_by_customers = df_order.groupby(['customer_id'],as_index=False).agg({'customer_order_rank':[max]})\n",
    "df_num_orders_by_customers.columns = ['customer_id','total_num_of_orders']\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(df_num_orders_by_customers['total_num_of_orders'], bins = 20)\n",
    "plt.title('Histogram of total number of orders by each customer', fontsize = 15, fontweight='bold' )\n",
    "plt.xlabel('Total orders by each customer', fontsize = 15)\n",
    "plt.ylabel('Count of customers', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, We can see that most of the customers has **ordered less than 20 orders** during 1st March, 2015 to 27th Feb, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Distribution of total amount paid by each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amount_paid_by_customers = df_order.groupby(['customer_id'],as_index=False).agg({'amount_paid':sum})\n",
    "df_amount_paid_by_customers.columns = ['customer_id','total_amount_paid']\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(df_amount_paid_by_customers['total_amount_paid'], bins = 30)\n",
    "plt.xlim([0, 1500])\n",
    "plt.title('Histogram of total amount paid by each customer', fontsize = 15, fontweight='bold' )\n",
    "plt.xlabel('Total amount paid each customer', fontsize = 15)\n",
    "plt.ylabel('Count of customers', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Details of Labeled Data <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get glimpse of labeled data\n",
    "print(df_labeled.shape)\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique customer ids in labeled data\n",
    "len(df_labeled['customer_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of customers returning/non-returning\n",
    "print('number of customers returning/non-returning: \\n{}\\n'.format(df_labeled['is_returning_customer'].value_counts()))\n",
    "#print('Fraction of customers returning/non-returning: \\n{}'.format(df_labeled['is_returning_customer'].value_counts()/len(df_labeled['is_returning_customer'])))\n",
    "plt.bar(['Non-Returning','Returning'],df_labeled['is_returning_customer'].value_counts()/len(df_labeled['is_returning_customer']))\n",
    "plt.title('Fraction of customers returning/non-returning ', fontsize = 15, fontweight='bold' )\n",
    "plt.xlabel('Customer Type', fontsize = 15)\n",
    "plt.ylabel('Fraction', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Engineering <a class=\"anchor\" id=\"section_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Failed Orders before Feature Engineering <a class=\"anchor\" id=\"section_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing failed orders\n",
    "df_successful_orders = df_order.loc[df_order['is_failed'] == 0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features <a class=\"anchor\" id=\"section_4_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of orders by each customer(Frequency (F) ) <a class=\"anchor\" id=\"section_4_2_1\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total orders by each customer\n",
    "df_total_orders = df_successful_orders.groupby(['customer_id'],as_index = False).agg({'customer_order_rank':[max]}) \n",
    "df_total_orders.columns = ['customer_id','total_orders']\n",
    "# merging df_total_orders with df_labeled data\n",
    "df_labeled = df_labeled.merge(df_total_orders, on = ['customer_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Check if percentage of returning customers is correlated with total orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating bins of total orders\n",
    "bins = [0, 10, 20, 30, max(df_labeled['total_orders'])]\n",
    "temp_df = df_labeled.copy()\n",
    "temp_df['total_order_bins'] = pd.cut(temp_df['total_orders'], bins)\n",
    "# bar plot \n",
    "perc_return_customers(temp_df, 'total_order_bins',bins,\"Percentage of returning customers with total orders\",\n",
    "'Total orders by customers','Percentage of returning customers','manual',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Above plot shows that Customers with higher number of total orders, have higher tendency to return. Total orders is a relevant/significant predictor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Order Value by Each Customer <a class=\"anchor\" id=\"section_4_2_2\"></a> \n",
    "`Check if percentage of returning customers is correlated with Total Order Value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total amount paid by each customer\n",
    "df_total_amount_paid = df_successful_orders.groupby(['customer_id'],as_index = False).agg({'amount_paid':sum})\n",
    "df_total_amount_paid = df_total_amount_paid.rename({'amount_paid':'total_amount_paid'},axis=1)\n",
    "df_labeled = df_labeled.merge(df_total_amount_paid,on=['customer_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating bins of total orders\n",
    "bins = [0, 50, 100, 150, 200 , 250, 300, max(df_labeled['total_amount_paid'])]\n",
    "temp_df = df_labeled.copy()\n",
    "temp_df['total_amount_paid_bins'] = pd.cut(temp_df['total_amount_paid'], bins)\n",
    "# bar plot \n",
    "plt.figure(figsize=(12,4))\n",
    "perc_return_customers(temp_df, 'total_amount_paid_bins',bins,\"Percentage of returning customers with total amount paid\",\n",
    "'Total amount paid by customers','Percentage of returning customers','manual',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Above plot shows that Customers with higher total amount paid, have higher tendency to return. Total amount paid is a relevant/significant predictor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total amount paid in last 180 days <a class=\"anchor\" id=\"section_4_2_3\"></a> \n",
    "`Check if percentage of returning customers is correlated with Total amount paid in last 180 days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Percentage of total purchase done in last 180 days\n",
    "last_date = df_successful_orders['order_date'].max()\n",
    "# create temporary dataframe for manipulation\n",
    "temp_df = df_successful_orders.copy()\n",
    "temp_df['days_before_last_date'] = (last_date - temp_df['order_date']).dt.days\n",
    "temp_df = temp_df.loc[temp_df['days_before_last_date'] <= 180,:].groupby(['customer_id'],as_index=False).agg({'amount_paid':sum})\n",
    "temp_df = temp_df.rename({'amount_paid':'amount_paid_in_last_180_days'},axis=1)\n",
    "temp_df.fillna(0, inplace=True)\n",
    "# merge with df_labeled\n",
    "df_labeled = df_labeled.merge(temp_df[['customer_id','amount_paid_in_last_180_days']], on = ['customer_id'], how = 'left')\n",
    "df_labeled['amount_paid_in_last_180_days'] = df_labeled['amount_paid_in_last_180_days'].fillna(0)\n",
    "temp_df = df_labeled.copy()\n",
    "temp_df['last_180_days_bins'] = pd.cut(temp_df['amount_paid_in_last_180_days'],4,include_lowest=True, precision=0)\n",
    "# bar plot \n",
    "plt.figure(figsize=(12,4))\n",
    "perc_return_customers(temp_df, 'last_180_days_bins',4,\"Total purchase in last 180 days\",\n",
    "'Total purchase in last 180 days','Percentage of returning customers','auto',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Above plot shows that Customers with higher total amount paid in last 180 days, have higher tendency to return. Total amount paid is a relevant/significant predictor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique restaurants customer ordered from <a class=\"anchor\" id=\"section_4_2_5\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Customers ordering from various restaurants may have higher likelihood of returning. Let's check`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = df_successful_orders.groupby(['customer_id','restaurant_id'], as_index=False).agg({'order_date':'count'})\n",
    "temp_df = temp_df.groupby(['customer_id'],as_index=False).size().reset_index(name='count')\n",
    "temp_df.columns = ['customer_id','num_unique_restaurants']\n",
    "df_labeled = df_labeled.merge(temp_df,on=['customer_id'],how = 'left')\n",
    "temp_df1 = df_labeled.copy()\n",
    "# creating bins for bar plot\n",
    "temp_df1['num_unique_restaurants_bins'] = pd.cut(temp_df1['num_unique_restaurants'],4,include_lowest=True, precision=0)\n",
    "# bar plot \n",
    "plt.figure(figsize=(12,4))\n",
    "perc_return_customers(temp_df1, 'num_unique_restaurants_bins',4,\"Number of unique restaurants customer ordered from\",\n",
    "'Number of unique restaurants','Percentage of returning customers','auto',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From Above plot, we can say that customers who orders from many different restaurants, have higher changes of returning. Numer of unique restaurants is relevant/significant predictor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recency, Customer Maturity, Percentage of spending in 2nd half <a class=\"anchor\" id=\"section_4_2_6\"></a> \n",
    "**Recency (R)** as days since last purchase: How many days ago was their last order? Deduct most recent order date from last date of dataset to calculate the recency value. 1 day ago? 14 days ago? 500 days ago?<br>\n",
    "**Customer Maturity** : Number of days between first order and last date of our data<br>\n",
    "**Percentage of spending in 2nd half** : Percentage of spending in 2nd half of a customer is total_amount_paid in 2nd half(m2) divided by total amount paid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **Recency(R)** and **Customer Maturity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculating Recency (R),Customer Maturity and Relative change in spending in 2nd half\n",
    "temp_df = df_successful_orders.copy() # create temporary copy to do manipulation\n",
    "temp_df['days_since_last_purchase'] = (last_date - temp_df['order_date']).dt.days\n",
    "temp_df = temp_df.groupby(['customer_id'],as_index = False).agg({'days_since_last_purchase':[min,max]})\n",
    "temp_df.columns = ['customer_id','recency','customer_maturity']\n",
    "df_labeled = df_labeled.merge(temp_df, on = ['customer_id'], how= 'left')\n",
    "temp_df = temp_df.merge(df_labeled[['customer_id','is_returning_customer']],on = ['customer_id'], how= 'inner')\n",
    "# creating bins of recency\n",
    "temp_df['recency_bins'] = pd.cut(temp_df['recency'],4,include_lowest=True,precision=0 )\n",
    "# creating bins of customer maturity\n",
    "temp_df['customer_maturity_bins'] = pd.cut(temp_df['customer_maturity'],4,include_lowest=True,precision=0 )\n",
    "# bar plot \n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(221,autoscaley_on = True)\n",
    "perc_return_customers(temp_df, 'recency_bins',4,\"Percentage of returning customers with Recency\",\n",
    "'Days Since Last Purchase (Recency)','Percentage of returning customers','auto',fontsize=11)\n",
    "plt.subplot(222,autoscaley_on = True)\n",
    "perc_return_customers(temp_df, 'customer_maturity_bins',4,\"Percentage of returning customers with Customer Maturity\",\n",
    "'Customer Maturity','Percentage of returning customers','auto',fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From above plots, we can see that (1) customrs with recent purchases are more likely to return (2) New customers (low maturity) as more likely to return as compared to more mature customers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **Percentage of spending in 2nd half**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Percentage of spending in 2nd half\n",
    "df_change = Percentage_spending_2nd_half(df_successful_orders,starting_date,last_date) #calculate change\n",
    "df_labeled = df_labeled.merge(df_change[['customer_id','perc_amount_paid_2nd_half']],on=['customer_id'],how='left')\n",
    "df_labeled['perc_amount_paid_2nd_half'] = df_labeled['perc_amount_paid_2nd_half'].fillna(0)\n",
    "temp_df = df_labeled.copy()\n",
    "temp_df['change_bins'] = pd.cut(temp_df['perc_amount_paid_2nd_half'],5,include_lowest=True,precision=0 ) # create bins\n",
    "#create bar plot\n",
    "perc_return_customers(temp_df, 'change_bins',5,\"Percentage of returning customers with Relative change\",\n",
    "'Relative change in spending in 2nd half','Percentage of returning customers','auto',fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From Above plot, We can say that people who spend more in 2nd half relative to 1st half, have more chances to return. \n",
    "Relative change in 2nd half is relevant/signifant predictor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum, Mean, Median Value of Orders <a class=\"anchor\" id=\"section_4_2_7\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_max_meam_median = df_successful_orders.groupby(['customer_id'],as_index=False).agg({'amount_paid':[max,'mean','median']})\n",
    "df_max_meam_median.columns = ['customer_id','max','mean','median']\n",
    "df_labeled = df_labeled.merge(df_max_meam_median,on=['customer_id'],how='left')\n",
    "df_max_meam_median = df_labeled.copy() \n",
    "df_max_meam_median['max_bins'] = pd.cut(df_max_meam_median['max'],5,include_lowest=True,precision=0 )\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.subplot(221,autoscaley_on = True)\n",
    "perc_return_customers(df_max_meam_median, 'max_bins',5,\"Percentage of returning customers with max order value\",\n",
    "'Max Order value','Percentage of returning customers','auto',fontsize=11)\n",
    "df_max_meam_median['mean_bins'] = pd.cut(df_max_meam_median['mean'],5,include_lowest=True,precision=0 )\n",
    "plt.subplot(222,autoscaley_on = True)\n",
    "perc_return_customers(df_max_meam_median, 'mean_bins',5,\"Percentage of returning customers with mean order value\",\n",
    "'Mean Order value','Percentage of returning customers','auto',fontsize=11)\n",
    "df_max_meam_median['median_bins'] = pd.cut(df_max_meam_median['median'],5,include_lowest=True,precision=0 )\n",
    "plt.subplot(223,autoscaley_on = True)\n",
    "perc_return_customers(df_max_meam_median, 'median_bins',5,\"Percentage of returning customers with median order value\",\n",
    "'Median Order value','Percentage of returning customers','auto',fontsize=11)\n",
    "plt.subplots_adjust(wspace = 0.2,hspace = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer Segmentation into Single Order,Normal,Attrition,At-Risk,Lost <a class=\"anchor\" id=\"section_4_2_8\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We will segment customers into single_order, normal, attrition, at_risk, lost based on their mean time units between orders and standard deviation of time units between orders as described below.\n",
    "only single purchase in dataset: single_order customer\n",
    "time since last order <= mean + 2*sd : Normal customer\n",
    "mean + 2*sd < time since last order <= mean + 4*sd : attrition customer\n",
    "mean + 4*sd < time since last order <= mean + 8*sd : at_risk customer\n",
    "time since last order > mean + 8*sd : lost customer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df_cust_classification = customer_classification(df_successful_orders)\n",
    "# merging customer classification with labeled data\n",
    "df_labeled = df_labeled.merge(df_cust_classification,on=['customer_id'], how = 'left')\n",
    "# calculating percentage of returning customers by each segment of customers\n",
    "temp_df = df_labeled.groupby(['customer_type'],as_index=False).agg({'customer_id':'count','is_returning_customer':'sum'})\n",
    "temp_df['perc_returning_cust'] = (temp_df['is_returning_customer']/temp_df['customer_id'])*100\n",
    "temp_df.sort_values(['perc_returning_cust'],ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting bar plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(temp_df['customer_type'],temp_df['perc_returning_cust'])\n",
    "plt.title('Percentage of returning customers in ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From above plot, we can see that single order and lost customers have minimum chances to return. Customer with attrition and at_risk also have changes to not-return. Customer classification is relevant/significant predictor for our model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Check if id columns correlated with Customer Return <a class=\"anchor\" id=\"section_4_2_9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check of Payment Id is correlated with Customer return\n",
    "print(perc_return_cust_by_ids(df_successful_orders,'payment_id'))\n",
    "print(perc_return_cust_by_ids(df_successful_orders,'platform_id'))\n",
    "print(perc_return_cust_by_ids(df_successful_orders,'transmission_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From above tables, We can say that Payment id, platform_id and transmission id doesn't have any correlation with likelihood of Customer Return. They are is not relevant predictors.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Creating New Features as Pairwise Products of Numerical Variables  <a class=\"anchor\" id=\"section_4_2_10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# numerical cols\n",
    "num_cols = ['total_orders','total_amount_paid','amount_paid_in_last_180_days','num_unique_restaurants','recency',\n",
    "            'customer_maturity','perc_amount_paid_2nd_half','max','mean','median']\n",
    "# get pairwise products of columns\n",
    "temp_df = pairwise_products(df_labeled[num_cols],num_cols)\n",
    "pairwise_df = temp_df.loc[:,(~pd.Series(temp_df.columns).isin(num_cols)).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Creating New Features as square of Numerical Variables  <a class=\"anchor\" id=\"section_4_2_11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "temp_df = square_numerical_cols(df_labeled[num_cols],num_cols)\n",
    "square_df = temp_df.loc[:,(~pd.Series(temp_df.columns).isin(num_cols)).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining pairwise and square columns with df_labeled data\n",
    "df_labeled = pd.concat([df_labeled,pairwise_df,square_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Modelling <a class=\"anchor\" id=\"section_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### One-Hot Encoding of Nominal Categorical Variables <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding of consumer_type variable\n",
    "df_labeled = pd.get_dummies(df_labeled,prefix_sep=\"_\",prefix = 'customer_type',columns=['customer_type'], dummy_na=False)\n",
    "df_labeled = df_labeled.fillna(0)\n",
    "df_labeled.reset_index(drop=True, inplace = True)\n",
    "# split X (predictor variables) and y (Target variable)\n",
    "X = df_labeled.loc[:,(~pd.Series(df_labeled.columns).isin(['customer_id','is_returning_customer'])).tolist()]\n",
    "y = df_labeled.loc[:,['is_returning_customer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train,Test Split of Dataset <a class=\"anchor\" id=\"section_5_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split Dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tackling imbalance class problem <a class=\"anchor\" id=\"section_5_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = undersampling_function(X_train,y_train,40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### XGBoost Model Development and Evaluation <a class=\"anchor\" id=\"section_5_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# create XGBoost model instance\n",
    "model_xgb = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 50)\n",
    "# Model fitting\n",
    "model_xgb.fit(X_train,y_train)\n",
    "# Model Prediction\n",
    "pred = model_xgb.predict(X_test)\n",
    "pred = pd.Series(pred)\n",
    "# get prediction label\n",
    "pred_label_xgboost = np.where(pred <= 0.5,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[46361 10576]\n",
      " [ 5454 11246]]\n",
      " accuracy: 0.7823105232423917\n",
      " sensitivity: 0.6734131736526946\n",
      " specificity: 0.814250838646223\n",
      " auc: 0.7438320061494588\n",
      " f1_score: 0.5838741498364571\n"
     ]
    }
   ],
   "source": [
    "cm,accuracy,sensitivity,specificity,auc,f1_score = model_evaluation_metrices(y_test['is_returning_customer'],pred_label_xgboost)\n",
    "print('confusion matrix: \\n{}\\n accuracy: {}\\n sensitivity: {}\\n specificity: {}\\n auc: {}\\n f1_score: {}'.format(cm,accuracy,sensitivity,specificity,auc,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Hyperparameter Grid Search <a class=\"anchor\" id=\"section_5_4\"></a> <a class=\"anchor\" id=\"section_5_4_1\"></a>\n",
    "***Run XGBoost Hyperparameter Grid Search only when you have lots of time. Grid Search takes more than 1hr to complete***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "param_comb = 5\n",
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'n_estimators':[50,100],\n",
    "        'min_child_weight': [1, 5],\n",
    "        'gamma': [0.5,1.5,5],\n",
    "        'subsample': [0.6, 1.0],\n",
    "        'colsample_bytree': [0.6,1.0],\n",
    "        'max_depth': [3,5]\n",
    "        }\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "grid = GridSearchCV(estimator=model_xgb, param_grid=params, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3,refit=True )\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Logistic Regression Model Development and Evaluation <a class=\"anchor\" id=\"section_5_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is required for logistic regression\n",
    "# X_train_normalized = preprocessing.normalize(X_train)\n",
    "# X_test_normalized = preprocessing.normalize(X_test)\n",
    "X_train_normalized=preprocessing.scale(X_train)\n",
    "X_test_normalized = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train_normalized, y_train['is_returning_customer'])\n",
    "# get Predictions\n",
    "predictions_lr = logisticRegr.predict(X_test_normalized)\n",
    "predictions_lr = pd.Series(predictions_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[45762 11175]\n",
      " [ 5328 11372]]\n",
      " accuracy: 0.7758871219631436\n",
      " sensitivity: 0.6809580838323354\n",
      " specificity: 0.8037304389061595\n",
      " auc: 0.7423442613692475\n",
      " f1_score: 0.5795092618544092\n"
     ]
    }
   ],
   "source": [
    "# get model evaluation metrices\n",
    "cm,accuracy,sensitivity,specificity,auc,f1_score = model_evaluation_metrices(y_test['is_returning_customer'],predictions_lr)\n",
    "print('confusion matrix: \\n{}\\n accuracy: {}\\n sensitivity: {}\\n specificity: {}\\n auc: {}\\n f1_score: {}'.format(cm,accuracy,sensitivity,specificity,auc,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Random Forest Model Development and Evaluation <a class=\"anchor\" id=\"section_5_6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=300, random_state=123)\n",
    "rf_model.fit(X_train,y_train['is_returning_customer'])\n",
    "# get predictions\n",
    "predictions_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[41023 15914]\n",
      " [ 5118 11582]]\n",
      " accuracy: 0.7143827152111031\n",
      " sensitivity: 0.6935329341317366\n",
      " specificity: 0.720498094385022\n",
      " auc: 0.7070155142583794\n",
      " f1_score: 0.5241198298488551\n"
     ]
    }
   ],
   "source": [
    "# get model evaluation metrices\n",
    "cm,accuracy,sensitivity,specificity,auc,f1_score = model_evaluation_metrices(y_test['is_returning_customer'],predictions_rf)\n",
    "print('confusion matrix: \\n{}\\n accuracy: {}\\n sensitivity: {}\\n specificity: {}\\n auc: {}\\n f1_score: {}'.format(cm,accuracy,sensitivity,specificity,auc,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
